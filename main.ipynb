{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Z37cZEN15qX",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install numpy matplotlib wfdb ipywidgets tsaug --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHCi6iIM2QWV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import wfdb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import tensorflow as tf\n",
        "from keras import layers, models, Model\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.signal import resample\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.metrics import f1_score\n",
        "from ipywidgets import interact, widgets\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, Input, Conv1D, BatchNormalization, Activation, Add, GlobalAveragePooling1D, Dense, MaxPooling1D, Dropout\n",
        "from keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBGFtp-e2T3g"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WGbeo7F0lQNr"
      },
      "outputs": [],
      "source": [
        "# Define the directory where the data is stored\n",
        "data_directory = '/content/drive/MyDrive/ctu-chb-intrapartum-cardiotocography-database-1.0.0'\n",
        "\n",
        "# List all .dat files in the directory\n",
        "dat_files = [f for f in os.listdir(data_directory) if f.endswith('.dat')]\n",
        "\n",
        "parameter_thresholds = {\n",
        "    \"pH\": (7.15,9999),\n",
        "    \"Apgar1\": (7, 999),\n",
        "    \"Apgar5\": (7, 999)\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualizing a sample (most distressed case)\n",
        "import wfdb\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Specify the record name\n",
        "record_name = \"2024\"  # Replace with the desired record name\n",
        "\n",
        "# Load the record\n",
        "record = wfdb.rdrecord(os.path.join(data_directory, record_name))\n",
        "\n",
        "# Extract FHR and UC signals\n",
        "fhr_signal = record.p_signal[:, record.sig_name.index('FHR')]\n",
        "uc_signal = record.p_signal[:, record.sig_name.index('UC')]\n",
        "\n",
        "# Print information about the record\n",
        "print(\"Record Information:\")\n",
        "print(\"Signals:\", record.sig_name)\n",
        "print(\"Units:\", record.units)\n",
        "\n",
        "# Get the metadata to obtain the correct units\n",
        "fhr_units = record.units[record.sig_name.index('FHR')]\n",
        "uc_units = record.units[record.sig_name.index('UC')]\n",
        "\n",
        "# Calculate the time vector based on the sampling frequency\n",
        "time_vector = (1 / record.fs) * np.arange(len(fhr_signal))\n",
        "\n",
        "# Set up a clean and professional plot with a professional color scheme\n",
        "plt.style.use('seaborn-darkgrid')\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Define professional colors\n",
        "fhr_color = '#3498db'  # Blue\n",
        "uc_color = '#e74c3c'   # Red\n",
        "\n",
        "# Plot FHR signal\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(time_vector, fhr_signal, color=fhr_color, label='FHR Signal', linewidth=1.5)\n",
        "plt.title('Fetal Heart Rate (FHR) Signal')\n",
        "plt.xlabel('Time (seconds)')\n",
        "plt.ylabel('Amplitude ({})'.format(fhr_units))\n",
        "plt.legend()\n",
        "\n",
        "# Plot UC signal\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(time_vector, uc_signal, color=uc_color, label='UC Signal', linewidth=1.5)\n",
        "plt.title('Uterine Contractions (UC) Signal')\n",
        "plt.xlabel('Time (seconds)')\n",
        "plt.ylabel('Amplitude (mmHg)')\n",
        "plt.legend()\n",
        "\n",
        "# Fine-tune layout for a professional look\n",
        "plt.tight_layout()\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kgHktuFIyfeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDVwI_UM2fZq"
      },
      "outputs": [],
      "source": [
        "# Initialize lists to corresponding labels\n",
        "labels = []\n",
        "\n",
        "# Process the uploaded .dat files and assign labels based on parameter thresholds\n",
        "for dat_file in dat_files:\n",
        "    record_name = os.path.splitext(dat_file)[0]  # Remove the .dat extension\n",
        "    record = wfdb.rdrecord(os.path.join(data_directory, record_name))\n",
        "\n",
        "    # Load the corresponding .hea file to access header information\n",
        "    hea_file_path = os.path.join(data_directory, record_name + '.hea')\n",
        "    with open(hea_file_path, 'r') as hea_file:\n",
        "        hea_content = hea_file.read()\n",
        "\n",
        "    # Check distress criteria based on thresholds\n",
        "    distress_flag = False\n",
        "    for param, (low, high) in parameter_thresholds.items():\n",
        "        param_value = float(hea_content.split(f\"#{param}\")[-1].split()[0])\n",
        "        if param_value < low:\n",
        "          distress_flag = True\n",
        "          break\n",
        "\n",
        "\n",
        "    # Assign labels based on distress flag\n",
        "    label = \"Distress\" if distress_flag else \"Normal\"\n",
        "    labels.append(label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgdn41_M2sqq"
      },
      "outputs": [],
      "source": [
        "# Convert binary labels to original format (\"Normal\" and \"Distress\")\n",
        "original_labels = np.array(labels)\n",
        "\n",
        "# Calculate label distribution\n",
        "unique_labels, label_counts = np.unique(original_labels, return_counts=True)\n",
        "\n",
        "# Create a pie chart\n",
        "fig, ax = plt.subplots()\n",
        "ax.pie(label_counts, labels=unique_labels, autopct='%1.1f%%', startangle=90)\n",
        "ax.axis('equal')\n",
        "\n",
        "plt.title('Distribution of Data')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import wfdb\n",
        "from scipy.signal import resample\n",
        "\n",
        "# Initialize variables to store maximum values\n",
        "max_fhr_bpm = 0\n",
        "max_uc_value = 0\n",
        "\n",
        "for dat_file in dat_files:\n",
        "    record_name = os.path.splitext(dat_file)[0]  # Remove the .dat extension\n",
        "    record = wfdb.rdrecord(os.path.join(data_directory, record_name))\n",
        "\n",
        "    # Extract FHR and UC signals\n",
        "    fhr_signal = record.p_signal[:, record.sig_name.index('FHR')]\n",
        "    uc_signal = record.p_signal[:, record.sig_name.index('UC')]\n",
        "\n",
        "    # Find maximum FHR BPM and UC value in the current record\n",
        "    max_fhr_bpm = max(max_fhr_bpm, np.max(fhr_signal))\n",
        "    max_uc_value = max(max_uc_value, np.max(uc_signal))\n",
        "\n",
        "# Initialize an empty list to store preprocessed signals\n",
        "X_signals = []\n",
        "\n",
        "for dat_file in dat_files:\n",
        "    record_name = os.path.splitext(dat_file)[0]  # Remove the .dat extension\n",
        "    record = wfdb.rdrecord(os.path.join(data_directory, record_name))\n",
        "\n",
        "    # Extract FHR and UC signals\n",
        "    fhr_signal = record.p_signal[:, record.sig_name.index('FHR')]\n",
        "    uc_signal = record.p_signal[:, record.sig_name.index('UC')]\n",
        "\n",
        "    # Resample signals to a common length\n",
        "    common_length = 1000\n",
        "    fhr_signal_resampled = resample(fhr_signal, common_length)\n",
        "    uc_signal_resampled = resample(uc_signal, common_length)\n",
        "\n",
        "    # Normalize FHR signal\n",
        "    fhr_signal_resampled /= max_fhr_bpm\n",
        "\n",
        "    # Normalize UC signal\n",
        "    uc_signal_resampled /= max_uc_value\n",
        "\n",
        "    # Combine FHR and UC signals into a single feature vector\n",
        "    combined_signal = np.concatenate((fhr_signal_resampled, uc_signal_resampled))\n",
        "\n",
        "    # Append the preprocessed signal to the list\n",
        "    X_signals.append(combined_signal)\n",
        "\n",
        "# Convert the list to a NumPy array for further processing\n",
        "X_signals = np.array(X_signals)\n"
      ],
      "metadata": {
        "id": "TEcooky5uoHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(max_fhr_bpm)\n",
        "print(max_uc_value)"
      ],
      "metadata": {
        "id": "gnKq2bxPxLTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqX-F_zj22Fm"
      },
      "outputs": [],
      "source": [
        "# Convert features and labels lists to NumPy arrays\n",
        "X_signals = np.array(X_signals)\n",
        "labels = np.array(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKh7Gbx524Uv"
      },
      "outputs": [],
      "source": [
        "X_signals.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pvqkn_CrluM0"
      },
      "outputs": [],
      "source": [
        "labels.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XEEC8dWhlxru"
      },
      "outputs": [],
      "source": [
        "import tsaug\n",
        "import numpy as np\n",
        "\n",
        "# Define the data augmentation pipeline\n",
        "augmenter = (\n",
        "    tsaug.AddNoise(scale=0.01) @ 0.9  # with 90% probability, introduce random jittering\n",
        ")\n",
        "\n",
        "# Increase the number of augmentations per sample to achieve a larger dataset\n",
        "num_augmentations_per_sample = 20  # Adjust this number as needed\n",
        "\n",
        "# Apply data augmentation to each sample in X_signals\n",
        "augmented_X_signals = []\n",
        "augmented_labels = []\n",
        "\n",
        "for i in range(X_signals.shape[0]):\n",
        "    original_signal = X_signals[i, :]\n",
        "\n",
        "    # Apply augmentation to the original signal multiple times\n",
        "    for _ in range(num_augmentations_per_sample):\n",
        "        augmented_signal = augmenter.augment(original_signal)\n",
        "        augmented_X_signals.append(augmented_signal)\n",
        "        augmented_labels.append(labels[i])\n",
        "\n",
        "# Reshape augmented data to match the original data shape\n",
        "augmented_X_signals_reshaped = np.array(augmented_X_signals).reshape(-1, X_signals.shape[1])\n",
        "\n",
        "# Concatenate original and augmented data\n",
        "X_signals_augmented = np.vstack((X_signals, augmented_X_signals_reshaped))\n",
        "labels_augmented = np.concatenate((labels, augmented_labels))\n",
        "\n",
        "# Shuffle the augmented dataset\n",
        "shuffle_indices = np.random.permutation(X_signals_augmented.shape[0])\n",
        "X_signals_augmented = X_signals_augmented[shuffle_indices]\n",
        "labels_augmented = labels_augmented[shuffle_indices]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.spatial.distance import euclidean\n",
        "\n",
        "# Select a specific sample index (wrap around if out of bounds)\n",
        "sample_index = 2018 % X_signals.shape[0]\n",
        "\n",
        "original_sample = X_signals[sample_index, :]\n",
        "augmented_samples = augmented_X_signals_reshaped[\n",
        "    sample_index * num_augmentations_per_sample : (sample_index + 1) * num_augmentations_per_sample, :\n",
        "]\n",
        "\n",
        "# Calculate Euclidean distances between original and augmented signals\n",
        "distances = [euclidean(original_sample, augmented_samples[i, :]) for i in range(num_augmentations_per_sample)]\n",
        "\n",
        "# Select the indices of the most distinct signals (e.g., top 3)\n",
        "most_distinct_indices = sorted(range(num_augmentations_per_sample), key=lambda i: distances[i], reverse=True)[:2]\n",
        "\n",
        "# Set Seaborn style\n",
        "sns.set(style=\"darkgrid\")\n",
        "\n",
        "# Plot the original and the most distinct augmented signals with different line styles and colors\n",
        "plt.figure(figsize=(20, 6))\n",
        "\n",
        "# Plot original signal\n",
        "plt.plot(original_sample, label='Original Signal', linewidth=2, color='black')\n",
        "\n",
        "# Different line styles and colors for augmented signals\n",
        "line_styles = ['--', '-.']\n",
        "colors = ['blue', 'orange']\n",
        "\n",
        "for i, idx in enumerate(most_distinct_indices):\n",
        "    plt.plot(augmented_samples[idx, :], label=f'Augment {i + 1}', linestyle=line_styles[i], color=colors[i], alpha=1)\n",
        "\n",
        "plt.xlabel('Feature Index')\n",
        "plt.ylabel('Signal Value')\n",
        "plt.legend()\n",
        "\n",
        "# Use Seaborn to enhance the plot\n",
        "sns.despine()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "RyBPDKUM18QJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RtoQJeR8l0ot"
      },
      "outputs": [],
      "source": [
        "X_signals_augmented.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-EPcdk1Ol5IK"
      },
      "outputs": [],
      "source": [
        "labels_augmented.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OcPAmDf9l8ey"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calculate class counts\n",
        "unique_labels, label_counts = np.unique(labels_augmented, return_counts=True)\n",
        "\n",
        "# Total number of samples in the augmented dataset\n",
        "total_samples = len(labels_augmented)\n",
        "\n",
        "# Plot pie chart with count values\n",
        "plt.figure(figsize=(4,4))\n",
        "plt.pie(label_counts, labels=unique_labels, autopct=lambda p: '{:.0f}'.format(p * total_samples / 100), startangle=140, textprops={'color': \"black\"})\n",
        "plt.title('Class Counts in Augmented Dataset')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tdp-NgHDmDEz"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming you have a DataFrame, adjust accordingly if using a different data structure\n",
        "import pandas as pd\n",
        "\n",
        "# Create a DataFrame for easy manipulation\n",
        "data = pd.DataFrame(data=X_signals_augmented)\n",
        "data['labels'] = labels_augmented\n",
        "\n",
        "# Separate distress and normal samples\n",
        "distress_data = data[data['labels'] == 'Distress']\n",
        "normal_data = data[data['labels'] == 'Normal']\n",
        "\n",
        "# Sample 1565 random normal samples\n",
        "balanced_normal_data = normal_data.sample(n=len(distress_data), random_state=42)\n",
        "\n",
        "# Concatenate distress and balanced normal samples\n",
        "balanced_data = pd.concat([distress_data, balanced_normal_data])\n",
        "\n",
        "# Shuffle the balanced dataset\n",
        "balanced_data = balanced_data.sample(frac=1, random_state=42)\n",
        "\n",
        "# Split the balanced dataset into features and labels\n",
        "X_balanced = balanced_data.drop(columns=['labels']).values\n",
        "labels_balanced = balanced_data['labels'].values\n",
        "\n",
        "# Split the balanced dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_balanced, labels_balanced, test_size=0.3, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGQfrUzHmYFZ"
      },
      "outputs": [],
      "source": [
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qnQRc_sumdbx"
      },
      "outputs": [],
      "source": [
        "# Encode labels to numerical values\n",
        "le = LabelEncoder()\n",
        "y_train_encoded = le.fit_transform(y_train)\n",
        "y_test_encoded = le.transform(y_test)\n",
        "\n",
        "# Reshape the data for CNN input\n",
        "X_train_cnn = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "X_test_cnn = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibJsxEzvgVFF"
      },
      "outputs": [],
      "source": [
        "from keras.regularizers import l2\n",
        "\n",
        "# Define hyperparameters\n",
        "initial_learning_rate = 0.00011\n",
        "batch_size = 64\n",
        "\n",
        "# Define dynamic learning rate callback\n",
        "reduce_lr =tf.keras.callbacks.ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.5,\n",
        "    patience=2,\n",
        "    verbose=1,\n",
        "    mode='auto'\n",
        ")\n",
        "# Modified model architecture\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv1D(32, kernel_size=3, activation='relu', input_shape=(X_train_cnn.shape[1], X_train_cnn.shape[2])))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Conv1D(64, kernel_size=3, activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.01)))  # Reduced the number of units\n",
        "model.add(Dropout(0.7))  # Increased dropout rate\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=initial_learning_rate), metrics=['accuracy'])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJDrUWShmoEV"
      },
      "outputs": [],
      "source": [
        "# Display the model summary\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nwRUGGgx0itP"
      },
      "outputs": [],
      "source": [
        "table=pd.DataFrame(columns=[\"Name\",\"Type\",\"Shape\"])\n",
        "for layer in model.layers:\n",
        "    table = table.append({\"Name\":layer.name, \"Type\": layer.__class__.__name__,\"Shape\":layer.output_shape}, ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q09L-OgX1Jp7"
      },
      "outputs": [],
      "source": [
        "table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAFzPO420hi-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming you have a DataFrame named 'table'\n",
        "table = pd.DataFrame(columns=[\"Layer\", \"Type\", \"Output Shape\"])\n",
        "\n",
        "for idx, layer in enumerate(model.layers):\n",
        "    layer_name = f\"{layer.__class__.__name__}_{idx + 1}\"\n",
        "    output_shape = layer.output_shape\n",
        "    table = table.append({\"Layer\": layer_name, \"Type\": layer.__class__.__name__, \"Output Shape\": output_shape}, ignore_index=True)\n",
        "\n",
        "# Add a row for the total params information\n",
        "total_params_row = {\"Layer\": \"Total params\", \"Type\": \"\", \"Output Shape\": \"2046657 (7.81 MB)\"}\n",
        "table = table.append(total_params_row, ignore_index=True)\n",
        "\n",
        "# Add a row for trainable params information\n",
        "trainable_params_row = {\"Layer\": \"Trainable params\", \"Type\": \"\", \"Output Shape\": \"2046465 (7.81 MB)\"}\n",
        "table = table.append(trainable_params_row, ignore_index=True)\n",
        "\n",
        "# Add a row for non-trainable params information\n",
        "non_trainable_params_row = {\"Layer\": \"Non-trainable params\", \"Type\": \"\", \"Output Shape\": \"192 (768.00 Byte)\"}\n",
        "table = table.append(non_trainable_params_row, ignore_index=True)\n",
        "\n",
        "# Convert the DataFrame to LaTeX table\n",
        "latex_table = table.to_latex(index=False, escape=False)\n",
        "\n",
        "# Print or save the LaTeX code\n",
        "print(latex_table)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model with dynamic learning rate\n",
        "history = model.fit(X_train_cnn, y_train_encoded,\n",
        "                    epochs=100,\n",
        "                    batch_size=batch_size,\n",
        "                    validation_data=(X_test_cnn, y_test_encoded),\n",
        "                    callbacks=[reduce_lr])"
      ],
      "metadata": {
        "id": "oXmcJVt_GbrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQtAlgYCmwCT"
      },
      "outputs": [],
      "source": [
        "model.save('/content/drive/MyDrive/fetaldistress.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PeDZmuQ80KqP"
      },
      "outputs": [],
      "source": [
        "from keras.utils import plot_model\n",
        "# Specify the filename for the generated image\n",
        "plot_model(model, to_file='model_architecture.png', show_shapes=True, show_layer_names=True)\n",
        "\n",
        "# Display the generated image\n",
        "from PIL import Image\n",
        "image = Image.open('model_architecture.png')\n",
        "image.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0zXG0LR1qKOx"
      },
      "outputs": [],
      "source": [
        "# Extract training history\n",
        "training_loss = history.history['loss']\n",
        "training_acc = history.history['accuracy']\n",
        "val_loss = history.history['val_loss']\n",
        "val_acc = history.history['val_accuracy']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILc8J0nCxf0w"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Extract learning rates from the history object\n",
        "learning_rates = history.history['lr']\n",
        "\n",
        "# Create a dataframe for Seaborn\n",
        "import pandas as pd\n",
        "df_lr = pd.DataFrame({\n",
        "    'Epoch': range(1, len(learning_rates) + 1),\n",
        "    'Learning Rate': learning_rates\n",
        "})\n",
        "\n",
        "# Plot learning rate changes\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.lineplot(data=df_lr, x='Epoch', y='Learning Rate', marker='o')\n",
        "plt.title('Learning Rate over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Learning Rate')\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwL_u5tXm_VW"
      },
      "outputs": [],
      "source": [
        "# Plot training history for accuracy\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "\n",
        "# Plot training history for loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTduRv21nApD"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "y_pred_prob = model.predict(X_test_cnn)\n",
        "y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "\n",
        "# Assuming 'le' is your LabelEncoder\n",
        "y_test_decoded = le.inverse_transform(y_test_encoded)\n",
        "y_pred_decoded = le.inverse_transform(y_pred.reshape(-1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cruS-Hh5up88"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# Confusion Matrix\n",
        "conf_mat = confusion_matrix(y_test_decoded, y_pred_decoded)\n",
        "\n",
        "# Plot Confusion Matrix\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=conf_mat, display_labels=le.classes_)\n",
        "disp.plot(cmap='Blues', values_format='d')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_aDuUR5tzte"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Generate classification report\n",
        "classification_rep = classification_report(y_test_decoded, y_pred_decoded)\n",
        "print(\"Classification Report:\\n\", classification_rep)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
